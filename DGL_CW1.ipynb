{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "jWAqfqlcc5O4",
   "metadata": {
    "id": "jWAqfqlcc5O4"
   },
   "source": [
    "# DGL CW1\n",
    "Hi everyone, good luck on the first CW of the DGL course. Please do not hesitate to ask your questions on EdStem if you have any.\n",
    "\n",
    "## Questions\n",
    "1. Centrality-based Graph Classification (25 points)\n",
    "2. Implementation of GraphSAGE with Node Sampling (30 points)\n",
    "3. Attention-based aggregation in node classification (30 points)\n",
    "4. Propagation rule integrating edge features/embeddings (15 points)\n",
    "5. Bonus Questions (5 points)\n",
    "\n",
    "## Submission Instructions\n",
    "For submission, you only need to submit your Jupyter Notebook file named \"CID.ipynb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3JNrkaurjfpM",
   "metadata": {
    "id": "3JNrkaurjfpM"
   },
   "source": [
    "# Required Libraries and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vZCmHCdxjn4I",
   "metadata": {
    "id": "vZCmHCdxjn4I"
   },
   "outputs": [],
   "source": [
    "%pip install dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DBqqR7fPjqKq",
   "metadata": {
    "id": "DBqqR7fPjqKq"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Data handling, numerical processing, and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning, neural network modules, and metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Graph processing\n",
    "import networkx as nx\n",
    "\n",
    "# DGL and datasets\n",
    "from dgl.data import PPIDataset, CoraGraphDataset\n",
    "\n",
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LQ7SGGtic5O6",
   "metadata": {
    "id": "LQ7SGGtic5O6"
   },
   "source": [
    "# 1) Centrality-based Graph Classification (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DeiwfbLOc5O7",
   "metadata": {
    "id": "DeiwfbLOc5O7"
   },
   "source": [
    "## Part 1: Implementing and Analyzing PageRank Centrality\n",
    "\n",
    "### Objective\n",
    "The goal of this task is to enhance your comprehension of the PageRank centrality algorithm through hands-on implementation. You are tasked with coding the algorithm from the ground up and applying it to two specific graphs, named $G1$ and $G2$. Following the application, you will analyze and compare the PageRank distributions between these two graphs.\n",
    "\n",
    "### Total Points: 10\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### 1. **Implement the PageRank Algorithm**\n",
    "\n",
    "- **Initialization**: Assign an equal PageRank to every node initially. For a graph containing $N$ nodes, each node's starting PageRank is $1/N$.\n",
    "\n",
    "- **Calculation**: Update each node's PageRank by employing the formula:\n",
    "  $$PR(p_i) = \\frac{1-d}{N} + d \\sum_{p_j \\in M(p_i)} \\frac{PR(p_j)}{L(p_j)}$$\n",
    "  where:\n",
    "  - $PR(p_i)$ denotes the PageRank of page $p_i$,\n",
    "  - $d$ represents the damping factor, usually set at 0.85,\n",
    "  - $N$ is the total count of nodes within the graph,\n",
    "  - $M(p_i)$ encompasses the set of nodes linking to $p_i$,\n",
    "  - $L(p_j)$ signifies the number of outbound links from node $p_j$.\n",
    "\n",
    "- **Convergence**: Continuously recalculate until the PageRank values reach a stable condition, evidenced by negligible changes between successive iterations.\n",
    "\n",
    "#### 2. **Pre-defined Graphs G1 and G2**\n",
    "\n",
    "- You have access to two graphs, $G1$ and $G2$, within your environment. These will serve as your experimental subjects for the PageRank algorithm application.\n",
    "\n",
    "#### 3. **Calculate PageRank Values**\n",
    "\n",
    "- Implement your PageRank algorithm on both $G1$ and $G2$ to determine the PageRank values for their respective nodes.\n",
    "\n",
    "#### 4. **Visualize the Results**\n",
    "\n",
    "- Create plots to showcase the PageRank value distributions for $G1$ and $G2$. Apply a transparent blue overlay for $G1$ and a transparent red for $G2$, facilitating an effortless comparison.\n",
    "\n",
    "#### 5. **Analysis**\n",
    "\n",
    "- **Reflect** upon the PageRank distribution variances between $G1$ and $G2$, pondering over:\n",
    "  - The impact of $G1$ and $G2$ structures on their PageRank distributions.\n",
    "  - Whether the PageRank values are more clustered or dispersed in one graph relative to the other.\n",
    "  - The presence of any nodes that emerge as notably more central within their graphs.\n",
    "\n",
    "### Comment Section\n",
    "\n",
    "- **Discuss** the insights and observations derived from juxtaposing the PageRank distributions of $G1$ and $G2$. Highlight any discerned patterns or anomalies, and deliberate on how each graph's inherent structure could elucidate these findings.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_page_rank(graph):\n",
    "    \"\"\"\n",
    "    Initialize the PageRank scores for all nodes in a graph.\n",
    "    \n",
    "    #TODO: Implement this function to initialize PageRank scores for each node.\n",
    "    - Assume each node has an equal initial PageRank value.\n",
    "    - The function takes a graph as input and returns a dictionary with nodes as keys and initial PageRank scores as values.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def calculate_page_rank(graph, d=0.85, max_iterations=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate the PageRank scores for all nodes in the graph using the iterative PageRank formula.\n",
    "    \n",
    "    #TODO: Implement this function to calculate the PageRank scores.\n",
    "    - Use the iterative PageRank algorithm with the damping factor `d`.\n",
    "    - Continue iterating until the change in PageRank scores is below `tol` or until `max_iterations` is reached.\n",
    "    - The function returns a dictionary with nodes as keys and their PageRank scores as values.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def visualize_distributions(page_ranks_g1, page_ranks_g2):\n",
    "    \"\"\"\n",
    "    Visualizes the distributions of PageRank values for two graphs on the same histogram for comparison.\n",
    "    \n",
    "    #TODO: Implement this function to visualize the distributions of PageRank values.\n",
    "    - Use a histogram to compare the PageRank distributions of two different graphs.\n",
    "    - Customize the plot with labels, titles, and a legend.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Generating larger graphs G1 and G2 with more complexity\n",
    "G1 = nx.fast_gnp_random_graph(100, 0.05, directed=True)\n",
    "G2 = nx.fast_gnp_random_graph(100, 0.05, directed=True)\n",
    "\n",
    "# Convert NetworkX graphs to adjacency lists\n",
    "graph_g1 = {n: list(G1.successors(n)) for n in G1.nodes()}\n",
    "graph_g2 = {n: list(G2.successors(n)) for n in G2.nodes()}\n",
    "\n",
    "#TODO: Use the `calculate_page_rank` function to calculate PageRank values for `graph_g1` and `graph_g2`.\n",
    "#TODO: Use the `visualize_distributions` function to compare the PageRank distributions of `graph_g1` and `graph_g2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b005c",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Graph Convolutional Networks (GCN) for Graph Classification\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your main objective is to modify the degree-based propagation rule using the following topological measures for normalization:\n",
    "1. **PageRank Centrality**: Utilize the PageRank centrality implementation from Part 1.\n",
    "2. **Betweenness Centrality**: Implement normalization using Betweenness centrality, which can be calculated using NetworkX.\n",
    "3. **Clustering Coefficient**: Implement normalization using the Clustering coefficient, available through NetworkX.\n",
    "\n",
    "For each normalization technique, you will develop a separate GCN model, resulting in four different GNN models including the original degree-based model.\n",
    "\n",
    "### Total Points: 15\n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### 1. **Implement a 2-Layer GCN**\n",
    "- **Graph Representation**: Begin with the simple node degree normalization for your initial GCN implementation. Specifically, employ the normalization technique $D^{-1}A$ where $D$ is the degree matrix and $A$ is the adjacency matrix of the graph.\n",
    "- **GCN Propagation Rule**: Implement the GCN layer using the updated propagation rule:\n",
    "  $$H^{(l+1)} = \\sigma(D^{-1}AH^{(l)}W^{(l)})$$\n",
    "  Here, $H^{(l)}$ represents the node features at layer $l$, $W^{(l)}$ is the weight matrix at layer $l$, and $\\sigma$ denotes a non-linear activation function, such as ReLU.\n",
    "- **Architecture**: Design your GCN with two convolutional layers following this propagation rule. Conclude with a Mean Pooling layer to aggregate node embeddings for graph-level prediction.\n",
    "- **Prediction Head**: Develop a prediction head that processes the pooled graph representation to classify the graph.\n",
    "\n",
    "#### 2. **Topological Measures for Normalization**\n",
    "Adapt the degree-based propagation rule to incorporate the following topological measures:\n",
    "- **PageRank Centrality**: Leverage the PageRank centrality implementation from Part 1.\n",
    "- **Betweenness Centrality and Clustering Coefficient**: Use NetworkX to compute these centrality measures for each node, applying them as normalization factors in the GCN propagation rule.\n",
    "\n",
    "#### 3. **Data Preparation**\n",
    "- Ensure the train and test sets are fixed to guarantee consistent evaluation across the different normalization techniques.\n",
    "\n",
    "#### 4. **Model Training and Evaluation**\n",
    "- Independently train a GCN model for each normalization technique.\n",
    "- Assess the performance of each model, focusing on accuracy, sensitivity, and specificity.\n",
    "\n",
    "#### 5. **Analysis of Embedding Distributions and Classification Results**\n",
    "- **Embedding Distributions**: Employ PCA to reduce the dimensionality of embeddings obtained from the final layer (Layer 2) of each GCN model, and visualize these distributions.\n",
    "- **Classification Results**: Contrast the accuracy, sensitivity, and specificity results among the four models.\n",
    "\n",
    "### Comment Section\n",
    "\n",
    "Engage in discussion on the following topics based on your analysis:\n",
    "- **Comparative Analysis**: Examine how the embedding distributions and classification outcomes differ with each normalization technique.\n",
    "- **Interpretation of Results**: Reflect on the performances of GCN models under different topological normalizations. Consider how PageRank centrality, betweenness centrality, and clustering coefficient influence the embeddings and classification capabilities of the models.\n",
    "\n",
    "Delve into the reasons behind the performances observed and theorize how the distinct topological characteristics of graphs may affect GCN model efficacy. Ponder the impact of each normalization method on feature propagation within the network and its capacity to harness the structural information of the graph for classification purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acedd480",
   "metadata": {},
   "source": [
    "## 1.1 Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8debb0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_molecule_graph(num_atoms=9, num_bonds=12):\n",
    "    \"\"\"\n",
    "    Creates a random molecular graph with a specified number of atoms and bonds.\n",
    "\n",
    "    Args:\n",
    "        num_atoms (int): The number of atoms in the molecule.\n",
    "        num_bonds (int): The number of bonds between atoms in the molecule.\n",
    "\n",
    "    Returns:\n",
    "        (nx.Graph, list): A tuple containing the generated molecular graph and the list of atom labels.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    atom_labels = [f\"{i}:{random.choice(['H', 'C', 'O'])}\" for i in range(num_atoms)]\n",
    "    G.add_nodes_from(atom_labels)\n",
    "\n",
    "    for _ in range(num_bonds):\n",
    "        atom1, atom2 = random.choice(atom_labels), random.choice(atom_labels)\n",
    "        while atom1 == atom2 or G.has_edge(atom1, atom2):\n",
    "            atom1, atom2 = random.choice(atom_labels), random.choice(atom_labels)\n",
    "        G.add_edge(atom1, atom2)\n",
    "\n",
    "    return G, atom_labels\n",
    "\n",
    "def classify_molecule(G):\n",
    "    \"\"\"\n",
    "    Classifies a molecule into type 1 or type 2 based on more complex criteria (without considering nitrogen).\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): The molecular graph.\n",
    "\n",
    "    Returns:\n",
    "        int: The type of molecule (0 or 1).\n",
    "    \"\"\"\n",
    "    atom_labels = list(G.nodes)\n",
    "    num_carbon = sum(1 for label in atom_labels if label.endswith(\":C\"))\n",
    "    num_oxygen = sum(1 for label in atom_labels if label.endswith(\":O\"))\n",
    "    num_hydrogen = sum(1 for label in atom_labels if label.endswith(\":H\"))\n",
    "    num_bonds = len(G.edges)\n",
    "\n",
    "    # Define more complex criteria for classification\n",
    "    if num_oxygen >= 2 and num_hydrogen >= 3 and num_carbon >= 2 and num_bonds >= 4:\n",
    "        return 1\n",
    "    elif num_oxygen >= 1 and num_carbon >= 3 and num_hydrogen >= 2 and num_bonds >= 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "def generate_balanced_molecule_dataset(num_samples_per_type):\n",
    "    \"\"\"\n",
    "    Generates a balanced dataset of random molecule graphs.\n",
    "\n",
    "    Args:\n",
    "        num_samples_per_type (int): The number of samples per molecule type.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, each containing a graph, its atom labels, and its classification.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    while len(dataset) < num_samples_per_type * 2:\n",
    "        G, atom_labels = create_random_molecule_graph()\n",
    "        classification = classify_molecule(G)\n",
    "        if classification in [0, 1]:\n",
    "            dataset.append((G, atom_labels, classification))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def graph_to_tensors(G, atom_labels, normalization_func=None):\n",
    "    \"\"\"\n",
    "    Converts a networkx graph of a molecule into tensor representations, with optional adjacency matrix normalization.\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): The molecular graph.\n",
    "        atom_labels (list): List of atom labels in the molecule.\n",
    "        normalization_func (callable, optional): Function to normalize the adjacency matrix.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the tensor representation of atom types (X) and (optionally normalized) adjacency matrix (A).\n",
    "    \"\"\"\n",
    "    atom_types = {'H': [1, 0, 0], 'C': [0, 1, 0], 'O': [0, 0, 1]}\n",
    "    X = torch.tensor([atom_types[node.split(':')[1]] for node in atom_labels], dtype=torch.float)\n",
    "\n",
    "    N = len(atom_labels)\n",
    "    A = torch.zeros((N, N), dtype=torch.float)\n",
    "    for i, j in G.edges:\n",
    "        idx1 = atom_labels.index(i)\n",
    "        idx2 = atom_labels.index(j)\n",
    "        A[idx1, idx2] = 1\n",
    "        A[idx2, idx1] = 1\n",
    "\n",
    "    if normalization_func:\n",
    "        A = normalization_func(G, A)  # Apply normalization if provided\n",
    "\n",
    "    return X, A\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for handling molecular graphs. Each sample in the dataset represents a molecule,\n",
    "    characterized by its graph structure and atom features, along with a classification label (e.g., for binary classification tasks).\n",
    "\n",
    "    The dataset initializes with a list of molecular data and optionally applies a normalization function to the adjacency matrix\n",
    "    of each molecule's graph. This normalization can be crucial for certain graph neural network models, affecting how the model\n",
    "    interprets the connectivity and flow of information through the graph.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (list): A list of tuples, where each tuple corresponds to a molecule and contains:\n",
    "                        - A graph representation of the molecule (e.g., a `networkx` graph).\n",
    "                        - Atom labels or features as a list or array.\n",
    "                        - A classification label for the molecule.\n",
    "        normalization_func (callable, optional): A function that takes an adjacency matrix (and potentially a graph) as input\n",
    "                        and returns a normalized adjacency matrix. This can be any normalization technique, such as degree normalization,\n",
    "                        PageRank-based normalization, etc.\n",
    "\n",
    "    The class provides two main methods as part of the PyTorch `Dataset` interface:\n",
    "    - `__len__` returns the number of items in the dataset.\n",
    "    - `__getitem__` retrieves a single item from the dataset by index, applying the normalization function if provided.\n",
    "\n",
    "    #TODO: Implement the `__init__` method to initialize the dataset with the given list and normalization function.\n",
    "    - Store the provided dataset list and normalization function as instance attributes.\n",
    "\n",
    "    #TODO: Implement the `__len__` method to return the size of the dataset.\n",
    "    - This method should simply return the length of the dataset list.\n",
    "\n",
    "    #TODO: Implement the `__getitem__` method to fetch and preprocess a single graph representation from the dataset.\n",
    "    - Extract the graph `G`, atom labels `atom_labels`, and classification `classification` for the specified index `idx`.\n",
    "    - Convert the graph `G` and `atom_labels` into tensor formats suitable for graph neural networks. This often involves creating a feature matrix `X` for nodes and an adjacency matrix `A`. Use the `graph_to_tensors` function (to be implemented separately by the student) for this conversion. Apply the `normalization_func` to the adjacency matrix if provided.\n",
    "    - Convert the `classification` label into a tensor of type `torch.long`.\n",
    "    - Return the feature matrix `X`, the (optionally normalized) adjacency matrix `A`, and the label tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, normalization_func=None):\n",
    "        #TODO: Initialize the dataset and the normalization function.\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        #TODO: Return the number of items in the dataset.\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: Process and return a single item from the dataset as tensors.\n",
    "        pass\n",
    "\n",
    "\n",
    "def prepare_data_loaders(num_samples_per_type, normalization_func=None, batch_size=10):\n",
    "    \"\"\"\n",
    "    Prepares DataLoader for training and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        num_samples_per_type (int): Number of samples per class to generate.\n",
    "        normalization_func (callable, optional): Normalization function to apply to adjacency matrices.\n",
    "        batch_size (int): Size of each data batch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of DataLoader: Training and testing DataLoader objects.\n",
    "    \"\"\"\n",
    "    dataset = generate_balanced_molecule_dataset(num_samples_per_type)\n",
    "    train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "    train_dataset = MoleculeDataset(train_dataset, normalization_func=normalization_func)\n",
    "    test_dataset = MoleculeDataset(test_dataset, normalization_func=normalization_func)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Your GCNLayer and GCN classes remain unchanged.\n",
    "\n",
    "# Generating a few random molecule graphs for visualization\n",
    "graphs = [create_random_molecule_graph() for _ in range(3)]\n",
    "\n",
    "# Plotting the generated graphs\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, (G, atom_labels) in enumerate(graphs):\n",
    "    pos = nx.spring_layout(G)  # Using spring layout for visual aesthetics\n",
    "    nx.draw(G, pos, with_labels=True, ax=axes[i])\n",
    "    axes[i].set_title(f\"Molecule Graph {i+1}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ba759",
   "metadata": {},
   "source": [
    "## 1.2 Visualization Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_omega_distribution(layers, epochs):\n",
    "    \"\"\"\n",
    "    Visualizes the distribution of weights (Omega) in each GCN layer of the Graph Neural Network across different epochs using smooth KDE plots.\n",
    "\n",
    "    Args:\n",
    "        layers (list of tuples): Each tuple contains a layer's name and a list of arrays representing the layer's weights at different epochs.\n",
    "        epochs (list of int): List of epoch numbers corresponding to the weight arrays.\n",
    "\n",
    "    This function plots the kernel density estimation (KDE) of weight values for each layer across specified epochs, allowing for the observation of how weight distributions evolve during training.\n",
    "    \"\"\"\n",
    "    for layer_name, weight_arrays in layers:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for i, weights in enumerate(weight_arrays):\n",
    "            omega_values = weights.flatten()  # Flatten the array to get a distribution of individual weight values\n",
    "            sns.kdeplot(omega_values, fill=True, label=f'Epoch {epochs[i]}')  # Use 'fill' for shaded KDE plots\n",
    "        plt.title(f'{layer_name} Weight Distribution Across Epochs', fontsize=16)\n",
    "        plt.xlabel('Weight Values', fontsize=12)\n",
    "        plt.ylabel('Density', fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_training_losses(losses_list, normalization_names):\n",
    "    \"\"\"\n",
    "    Plots the training loss over epochs for different normalization techniques.\n",
    "\n",
    "    Args:\n",
    "        losses_list (list of lists): Each sublist contains the training losses for one normalization technique over all epochs.\n",
    "        normalization_names (list of str): Names of the normalization techniques used.\n",
    "\n",
    "    This function creates a line plot for each normalization technique's training loss over epochs, facilitating comparison of their performance.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, train_losses in enumerate(losses_list):\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label=f'{normalization_names[i]} Normalization')\n",
    "    plt.title('Training Loss Over Epochs for Different Normalization Techniques', fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metric_bar_charts(names, metric_values, metric_names):\n",
    "    \"\"\"\n",
    "    Creates bar charts for different evaluation metrics across various normalization techniques.\n",
    "\n",
    "    Args:\n",
    "        names (list of str): Names of the normalization techniques.\n",
    "        metric_values (list of lists): Each sublist contains the values of a metric for each normalization technique.\n",
    "        metric_names (list of str): Names of the metrics being plotted.\n",
    "\n",
    "    This function plots a bar chart for each provided metric, comparing the performance of different normalization techniques,\n",
    "    with y-axis limits dynamically adjusted to emphasize differences while capping at 1.\n",
    "    \"\"\"\n",
    "    num_metrics = len(metric_names)\n",
    "    num_rows = num_cols = int(math.ceil(math.sqrt(num_metrics)))\n",
    "\n",
    "    sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12 * num_cols, 8 * num_rows))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    for i, metric_name in enumerate(metric_names):\n",
    "        ax = axes.flatten()[i] if num_metrics > 1 else axes\n",
    "\n",
    "        # Create DataFrame for seaborn\n",
    "        data = pd.DataFrame({\n",
    "            'Normalization Technique': np.repeat(names, len(metric_values[i])),\n",
    "            metric_name: np.concatenate([metric_values[i] for _ in names])\n",
    "        })\n",
    "\n",
    "        sns.barplot(x='Normalization Technique', y=metric_name, data=data, ax=ax, alpha=0.75)\n",
    "\n",
    "        # Dynamically adjust the y-axis limits\n",
    "        min_val = min(data[metric_name]) * 0.9  # Start slightly below the smallest value for better visibility\n",
    "        max_val = 1  # Ensuring the upper limit is 1\n",
    "        ax.set_ylim([min_val, max_val])\n",
    "\n",
    "        ax.set_xlabel('Normalization Technique', fontsize=14)\n",
    "        ax.set_ylabel(f'{metric_name} Value', fontsize=14)\n",
    "        ax.set_title(f'Comparison of {metric_name}', fontsize=16)\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        # Add text labels above bars\n",
    "        for p, value in zip(ax.patches, np.concatenate([metric_values[i] for _ in names])):\n",
    "            ax.text(p.get_x() + p.get_width() / 2., p.get_height(), f'{value:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    # Hide unused subplots if the number of metrics is less than the number of subplot positions\n",
    "    for i in range(num_metrics, num_rows * num_cols):\n",
    "        if num_rows * num_cols == 1:\n",
    "            break\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def extract_embeddings(model, loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Extracts embeddings from a model given a data loader.\n",
    "\n",
    "    #TODO: Implement this function to extract embeddings from the provided model using data from the loader.\n",
    "    - Set the model to evaluation mode.\n",
    "    - Initialize lists or arrays to store embeddings and labels.\n",
    "    - Iterate over batches of data from the loader, ensuring to move the data to the specified device.\n",
    "    - For each batch, use the model to compute embeddings. If the model requires specific inputs (e.g., features and adjacency matrix), ensure they are correctly passed.\n",
    "    - Apply necessary post-processing on embeddings (e.g., mean pooling) and convert them to a suitable format (e.g., numpy array) for further analysis or visualization.\n",
    "    - Collect and store the labels associated with each embedding for potential use in tasks like visualization or analysis.\n",
    "    - Return the embeddings and labels as a tuple. Ensure embeddings are in a continuous array format suitable for analysis.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model from which to extract embeddings.\n",
    "        loader (DataLoader): DataLoader providing batches of data for embedding extraction.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements. The first is a numpy array of embeddings, and the second is a list of labels associated with each embedding.\n",
    "\n",
    "    Note: This function should handle device placement (CPU or GPU) for both the data and model, and ensure gradients are not computed to optimize memory and compute resources.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def apply_pca_and_visualize_all_sns(embeddings_list, titles, labels_list):\n",
    "    \"\"\"\n",
    "    Applies PCA to reduce dimensionality of embeddings and visualizes them using scatter plots.\n",
    "\n",
    "    Args:\n",
    "        embeddings_list (list of np.ndarray): List of embeddings arrays to be visualized.\n",
    "        titles (list of str): Titles for each subplot, typically representing the condition or category of the embeddings.\n",
    "        labels_list (list of np.ndarray): List of label arrays corresponding to the embeddings for coloring the points.\n",
    "\n",
    "    This function reduces embeddings to two principal components using PCA and plots them, coloring points by their labels to distinguish between categories.\n",
    "    \"\"\"\n",
    "    sns.set(style='whitegrid')\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (embeddings, title, labels) in enumerate(zip(embeddings_list, titles, labels_list)):\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_embeddings = pca.fit_transform(embeddings)\n",
    "        df = pd.DataFrame(data=pca_embeddings, columns=['PCA1', 'PCA2'])\n",
    "        df['Label'] = labels  # Add labels for coloring\n",
    "\n",
    "        sns.scatterplot(ax=axs[i], x='PCA1', y='PCA2', hue='Label', data=df, palette='viridis', alpha=0.7).set_title(title)\n",
    "        axs[i].set_xlabel('PCA Component 1')\n",
    "        axs[i].set_ylabel('PCA Component 2')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a793cd23",
   "metadata": {},
   "source": [
    "## 1.3 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ecd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single Graph Convolutional Layer.\n",
    "\n",
    "    #TODO: Implement a GCN layer that performs graph convolution. This layer should first apply a linear transformation\n",
    "    to the node features and then utilize the adjacency matrix to incorporate neighborhood information. You will need to\n",
    "    define and initialize a weight matrix for the linear transformation of node features.\n",
    "    \n",
    "    Attributes:\n",
    "        - Define an attribute for the weight matrix.\n",
    "\n",
    "    Args:\n",
    "        - Accept the number of input features per node.\n",
    "        - Accept the number of output features per node.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        #TODO: Initialize the weight matrix as a torch.nn.Parameter.\n",
    "        pass\n",
    "\n",
    "    def init_parameters(self):\n",
    "        #TODO: Implement a method to initialize the weights uniformly with a standard deviation based on layer size.\n",
    "        pass\n",
    "\n",
    "    def forward(self, input, adjacency):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN layer.\n",
    "        \n",
    "        #TODO: Implement the forward pass method. Apply a linear transformation to the input features and then\n",
    "        use the adjacency matrix to incorporate neighborhood information. The method should return the output feature\n",
    "        matrix after graph convolution.\n",
    "        \n",
    "        Args:\n",
    "            - input: Input feature matrix where each row represents node features.\n",
    "            - adjacency: Adjacency matrix of the graph.\n",
    "        \n",
    "        Returns:\n",
    "            - Output feature matrix after applying the graph convolution.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Graph Convolutional Network (GCN) for node classification.\n",
    "\n",
    "    #TODO: Implement a GCN model for node classification. The model should consist of two GCN layers followed by a\n",
    "    global mean pooling and a fully connected layer for classification. You need to define the GCN layers and the fully\n",
    "    connected layer in the constructor.\n",
    "    \n",
    "    Args:\n",
    "        - nfeat: Number of features for each input node.\n",
    "        - nhid: Number of hidden units for each GCN layer.\n",
    "        - nclass: Number of classes (output dimension).\n",
    "    \"\"\"\n",
    "    def __init__(self, nfeat, nhid, nclass):\n",
    "        super(GCN, self).__init__()\n",
    "        #TODO: Define the first and second graph convolutional layers and the fully connected layer for classification.\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, adj, return_embedding=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN.\n",
    "        \n",
    "        #TODO: Implement the forward pass. Apply two GCN layers with ReLU activation, perform global mean pooling,\n",
    "        and then use a fully connected layer for classification. If `return_embedding` is True, return the embedding\n",
    "        from the second GCN layer before classification.\n",
    "        \n",
    "        Args:\n",
    "            - x: Input feature matrix where each row is the feature vector of a node.\n",
    "            - adj: Adjacency matrix of the graph.\n",
    "            - return_embedding: If True, returns the embedding from the second GCN layer before classification.\n",
    "        \n",
    "        Returns:\n",
    "            - The output log softmax probabilities for node classification if return_embedding is False, otherwise\n",
    "            returns the embeddings from the second GCN layer.\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a96b83",
   "metadata": {},
   "source": [
    "## 1.4 Normalization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_degree_matrix_normalization(G, adjacency):\n",
    "    \"\"\"\n",
    "    Computes the degree matrix normalization D^-1 * A for the given graph.\n",
    "\n",
    "    #TODO: Implement this function to normalize the adjacency matrix by the inverse degree of each node.\n",
    "    - Calculate the degree for each node.\n",
    "    - Compute the inverse degree matrix D^-1.\n",
    "    - Normalize the adjacency matrix using D^-1 * A.\n",
    "    - Ensure to handle cases with isolated nodes by adding a small epsilon to the degrees to prevent division by zero.\n",
    "    - Convert and return the normalized adjacency matrix as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_pagerank_normalization(G, adjacency):\n",
    "    \"\"\"\n",
    "    Normalizes the adjacency matrix using PageRank centrality values.\n",
    "\n",
    "    #TODO: Implement this function to apply PageRank normalization on the adjacency matrix.\n",
    "    - Compute PageRank values for each node in the graph.\n",
    "    - Create a diagonal matrix with PageRank values.\n",
    "    - Normalize the adjacency matrix using the PageRank diagonal matrix.\n",
    "    - Convert and return the normalized adjacency matrix as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_betweenness_normalization(G, adjacency):\n",
    "    \"\"\"\n",
    "    Normalizes the adjacency matrix using Betweenness centrality values.\n",
    "\n",
    "    #TODO: Implement this function to utilize Betweenness centrality for adjacency matrix normalization.\n",
    "    - Calculate Betweenness centrality for each node.\n",
    "    - Construct a diagonal matrix using the centrality values.\n",
    "    - Apply this matrix to normalize the adjacency matrix.\n",
    "    - Convert and return the normalized adjacency matrix as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_clustering_coefficient_normalization(G, adjacency):\n",
    "    \"\"\"\n",
    "    Normalizes the adjacency matrix using Clustering coefficient values.\n",
    "\n",
    "    #TODO: Implement this function to leverage Clustering coefficients for adjacency matrix normalization.\n",
    "    - Compute the Clustering coefficient for each node.\n",
    "    - Form a diagonal matrix with these coefficients.\n",
    "    - Normalize the adjacency matrix using this coefficient matrix.\n",
    "    - Convert and return the normalized adjacency matrix as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e56d0",
   "metadata": {},
   "source": [
    "## 1.5 Training & Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, epochs=100, device='cpu'):\n",
    "    \"\"\"\n",
    "    Trains the model over a specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to be trained.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for the training data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer used for model parameter updates.\n",
    "        criterion (torch.nn.Module): Loss function used for training.\n",
    "        epochs (int, optional): Number of epochs to train the model. Defaults to 100.\n",
    "        device (str, optional): The device to run the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the average loss value for each epoch.\n",
    "\n",
    "    This function iterates over the training dataset for a given number of epochs, performing\n",
    "    forward and backward passes, and updates the model parameters. The average loss per epoch is recorded and returned.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_values = []  # Initialize a list to store the average loss per epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0  # Track total loss for each epoch\n",
    "\n",
    "        for X, A, labels in train_loader:\n",
    "            # Move data to the specified device\n",
    "            X, A, labels = X.to(device), A.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear gradients for the next train step\n",
    "            output = model(X, A)  # Forward pass\n",
    "\n",
    "            loss = criterion(output, labels)  # Compute the loss\n",
    "            loss.backward()  # Backward pass to compute gradients\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)  # Calculate average loss\n",
    "        loss_values.append(avg_loss)  # Append average loss to list\n",
    "\n",
    "        # Print the average loss for the current epoch\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model to be evaluated.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for the test data.\n",
    "        device (str, optional): The device to run the model on ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the accuracy, precision, recall, and F1 score of the model on the test dataset.\n",
    "\n",
    "    This function performs a forward pass on the test dataset to obtain the model's predictions,\n",
    "    then calculates and returns various evaluation metrics including accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    true_labels = []  # List to store actual labels\n",
    "    predictions = []  # List to store model predictions\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for X, A, labels in test_loader:\n",
    "            # Move data to the specified device\n",
    "            X, A, labels = X.to(device), A.to(device), labels.to(device)\n",
    "\n",
    "            output = model(X, A)  # Forward pass\n",
    "            _, predicted = torch.max(output.data, 1)  # Get the index of the max log-probability\n",
    "\n",
    "            true_labels += labels.tolist()  # Append actual labels\n",
    "            predictions += predicted.tolist()  # Append predicted labels\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484968ac",
   "metadata": {},
   "source": [
    "## 1.6 Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to train and evaluate Graph Convolutional Network (GCN) models\n",
    "    with different graph normalization techniques, visualize training metrics, and perform\n",
    "    embedding analysis through PCA.\n",
    "\n",
    "    Assumes the presence of a GCN model class, data loader preparation functions, and\n",
    "    various normalization technique functions defined outside this script.\n",
    "    \"\"\"\n",
    "    # Configuration parameters\n",
    "    num_samples_per_type = 1000  # Number of samples per class/type\n",
    "    num_epochs = 200  # Number of training epochs\n",
    "    # Dictionary mapping normalization technique names to their corresponding functions\n",
    "    normalization_techniques = {\n",
    "        'degree': compute_degree_matrix_normalization,\n",
    "        'pagerank': compute_pagerank_normalization,\n",
    "        'betweenness': compute_betweenness_normalization,\n",
    "        'clustering': compute_clustering_coefficient_normalization,\n",
    "    }\n",
    "\n",
    "    # Lists for storing evaluation metrics and model information\n",
    "    metric_values = [[] for _ in range(4)]  # Lists to store Accuracy, Precision, Recall, F1 Score\n",
    "    normalization_names = []  # Names of the normalization techniques\n",
    "    train_losses_list = []  # Training loss values for each normalization technique\n",
    "    models = []  # Trained models\n",
    "\n",
    "    # Set the computation device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Loop over each normalization technique to train and evaluate a model\n",
    "    for name, norm_func in normalization_techniques.items():\n",
    "        print(f\"\\nTraining model with {name} normalization...\")\n",
    "        # Prepare data loaders\n",
    "        train_loader, test_loader = prepare_data_loaders(num_samples_per_type, normalization_func=norm_func, batch_size=50)\n",
    "        print(f\"DataLoader batch size: {train_loader.batch_size}\")\n",
    "\n",
    "        # Initialize the GCN model, optimizer, and loss criterion\n",
    "        model = GCN(nfeat=3, nhid=16, nclass=2).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train the model\n",
    "        train_losses = train_model(model, train_loader, optimizer, criterion, epochs=num_epochs, device=device)\n",
    "        train_losses_list.append(train_losses)\n",
    "\n",
    "        # Store the trained model\n",
    "        models.append(model)\n",
    "\n",
    "        # Evaluate the model's performance\n",
    "        accuracy, precision, recall, f1 = evaluate_model(model, test_loader, device=device)\n",
    "        # Store the evaluation metrics\n",
    "        metric_values[0].append(accuracy)\n",
    "        metric_values[1].append(precision)\n",
    "        metric_values[2].append(recall)\n",
    "        metric_values[3].append(f1)\n",
    "        normalization_names.append(name)\n",
    "\n",
    "        # Output the evaluation results\n",
    "        print(f\"Results with {name} normalization - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Visualization of training losses and evaluation metrics for each normalization technique\n",
    "    metric_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    plot_training_losses(train_losses_list, normalization_names)\n",
    "    plot_metric_bar_charts(normalization_names, metric_values, metric_names)\n",
    "\n",
    "    # Embedding extraction and PCA visualization\n",
    "    embeddings_list = []\n",
    "    labels_list = []  # Labels for each set of embeddings\n",
    "    titles = []  # Titles for the PCA plots\n",
    "\n",
    "    # Extract embeddings and labels for each model\n",
    "    for name, model in zip(normalization_names, models):\n",
    "        print(f\"\\nExtracting embeddings for model trained with {name} normalization...\")\n",
    "        embeddings, labels = extract_embeddings(model, test_loader, device=device)\n",
    "        embeddings_list.append(embeddings)\n",
    "        labels_list.append(labels)  # Append corresponding labels\n",
    "        titles.append(f\"Embedding Distributions with {name} normalization\")\n",
    "\n",
    "    # Apply PCA and visualize the embeddings\n",
    "    apply_pca_and_visualize_all_sns(embeddings_list, titles, labels_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xg8ZtRfK463d",
   "metadata": {
    "id": "Xg8ZtRfK463d"
   },
   "source": [
    "# 2) Implementation of GraphSAGE with Node Sampling (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cn37bADeLk0h",
   "metadata": {
    "id": "cn37bADeLk0h"
   },
   "source": [
    "## 2.1) Dataset\n",
    "\n",
    "In this question, we are going to train and test GraphSAGE on a **node classification** task using a toy Protein-Protein Interaction (PPI) dataset.\n",
    "\n",
    "The dataset contains 24 graphs. The average number of nodes per graph is 2372. Each node has 50 features and 121 labels.\n",
    "\n",
    "Since we will work on a node classification task, we will select only one of the graphs.\n",
    "\n",
    "Below, we load the dataset and split into train/validation/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pKOlr5XwLoaw",
   "metadata": {
    "id": "pKOlr5XwLoaw"
   },
   "outputs": [],
   "source": [
    "def load_ppi_data():\n",
    "    # Load the dataset\n",
    "    dataset = PPIDataset()\n",
    "\n",
    "    # Select one graph from the PPI dataset\n",
    "    g = dataset[0]\n",
    "\n",
    "    # Extract features, labels\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "\n",
    "    num_nodes = g.number_of_nodes()\n",
    "    num_train = int(0.6 * num_nodes)  # 60% for training\n",
    "    num_val = int(0.2 * num_nodes)    # 20% for validation\n",
    "\n",
    "    # Create a random permutation of node indices\n",
    "    indices = torch.randperm(num_nodes)\n",
    "\n",
    "    # Assign the first num_train nodes to the training set\n",
    "    # Assign the next num_val nodes to the validation set\n",
    "    # Assign the remaining nodes to the test set\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[indices[:num_train]] = True\n",
    "    val_mask[indices[num_train:num_train+num_val]] = True\n",
    "    test_mask[indices[num_train+num_val:]] = True\n",
    "\n",
    "    adj = g.adjacency_matrix().to_dense()\n",
    "\n",
    "    return features, labels, adj, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rdp7_8kzLpZJ",
   "metadata": {
    "id": "Rdp7_8kzLpZJ"
   },
   "outputs": [],
   "source": [
    "features, labels, adj, train_mask, val_mask, test_mask = load_ppi_data()\n",
    "\n",
    "features = features.to(device)\n",
    "labels = labels.to(device)\n",
    "adj = adj.to(device)\n",
    "\n",
    "num_feats = features.shape[1]\n",
    "num_classes = labels.shape[1]\n",
    "\n",
    "# Convert one-hot encoding to class indices format\n",
    "# e.g.,\n",
    "# one-hot encoding vector [0, 0, 1, 0, 0, ....] is converted to class index 2.\n",
    "# To use torch.nn.CrossEntropyLoss, we need to have this class indices format. \n",
    "# For more information, check https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "train_features = features[train_mask]\n",
    "val_features = features[val_mask]\n",
    "test_features = features[test_mask]\n",
    "\n",
    "train_labels = labels[train_mask]\n",
    "val_labels = labels[val_mask]\n",
    "test_labels = labels[test_mask]\n",
    "\n",
    "train_adj = adj[train_mask][:, train_mask]\n",
    "val_adj = adj[val_mask][:, val_mask]\n",
    "test_adj = adj[test_mask][:, test_mask]\n",
    "\n",
    "print(f\"Number of train nodes: {train_adj.shape[0]}\")\n",
    "print(f\"Number of val nodes: {val_adj.shape[0]}\")\n",
    "print(f\"Number of test nodes: {test_adj.shape[0]}\")\n",
    "print(f\"Number of features: {num_feats}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FCGjLT2KfEPF",
   "metadata": {
    "id": "FCGjLT2KfEPF"
   },
   "outputs": [],
   "source": [
    "def compute_average_degree(A):\n",
    "    degrees = A.sum(dim=1)  # Sum along rows to get degrees\n",
    "    average_degree = degrees.mean().item()  # Compute the mean degree\n",
    "\n",
    "    return average_degree\n",
    "\n",
    "average_degree = compute_average_degree(train_adj)\n",
    "print(\"Average Degree:\", average_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MiyIOQSXMgqB",
   "metadata": {
    "id": "MiyIOQSXMgqB"
   },
   "source": [
    "## 2.2) Node-wise Sampling (15 points)\n",
    "Node-wise sampling involves aggregating a subset of neighbors for each node in the graph, as opposed to considering all neighbors for aggregation (see the Figure 1). \n",
    "\n",
    "In the GraphSAGE, they sample a fixed number of neighbors in each layer. More specifically, they use $K=2$ number of layers and for the first layer and second layer, they sample $S_1=25$ and $S_2=10$ neighbors, respectively. \n",
    "\n",
    "**Here, for simplicity, we will sample $S=S_1=S_2=5$ neighbors for both layers**.\n",
    "\n",
    "<img src=\"figures/nodewise_sampling.jpg\" alt=\"Node-wise sampling\" width=\"200\" />\n",
    "\n",
    "(Figure 1: Node-wise sampling<sup>1</sup>)\n",
    "\n",
    "<sup>1</sup>_Hamilton, W., Ying, Z., & Leskovec, J. (2017). Inductive representation learning on large graphs. Advances in neural information processing systems, 30._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zdm91LJaOMp3",
   "metadata": {
    "id": "Zdm91LJaOMp3"
   },
   "source": [
    "Below, you need to implement the sampler function. It takes the adj. matrix A and number of neighbors to sample, returns a list of lists including indices to sampled neighbors for each node in A.\n",
    "\n",
    "You can use [torch.randperm](https://pytorch.org/docs/stable/generated/torch.randperm.html) or feel free to use any other function that does the same job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OA2mQO3GOliw",
   "metadata": {
    "id": "OA2mQO3GOliw"
   },
   "outputs": [],
   "source": [
    "def sampler(A, num_samples):\n",
    "    \"\"\"\n",
    "    Samples \"num_samples\" amount of neighbors for each node in adj. matrix A\n",
    "    You can use uniform random sampling. No need for any importance sampling strategy.\n",
    "\n",
    "    Params:\n",
    "        A (Tensor): Adj. matrix of shape (N x N)\n",
    "        num_samples (int): Number of neighbors to sample for each node\n",
    "        where N is the number of nodes.\n",
    "\n",
    "    Returns:\n",
    "        A list of lists including indices to sampled neighbors for each node in A.\n",
    "    \"\"\"\n",
    "\n",
    "    N = A.shape[0]  # Number of nodes\n",
    "    sampled_neighbors = []\n",
    "\n",
    "    ########## YOUR CODE HERE ##########\n",
    "    # For each node, populate the sampled_neighors list\n",
    "    # Here is a dummy example with num_samples = 3\n",
    "    # sampled_neighbors = [[3, 41, 2], [53, 234], ...]\n",
    "    # for the first node, the indices for sampled neighbors are 3, 41 and 2.\n",
    "    # the second node has only 2 neighbors (smaller than the num_samples), thus we sampled all its neighbors.\n",
    "    ####################################\n",
    "\n",
    "    return sampled_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HjtPeehpQIVP",
   "metadata": {
    "id": "HjtPeehpQIVP"
   },
   "source": [
    "Let's see the sampled neighbors for the first 10 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YfGkkONoPFo3",
   "metadata": {
    "id": "YfGkkONoPFo3"
   },
   "outputs": [],
   "source": [
    "# set num_samples to 2 just for now\n",
    "num_samples = 2\n",
    "\n",
    "sampled_neighbors = sampler(adj[:10], num_samples)\n",
    "\n",
    "# Print the sampled neighbors for each node\n",
    "for node, neighbors in enumerate(sampled_neighbors):\n",
    "    print(f\"Node {node}: Sampled Neighbors {neighbors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "It_a42RolM_h",
   "metadata": {
    "id": "It_a42RolM_h"
   },
   "source": [
    "## 2.3) Implementation of GraphSAGE (15 points)\n",
    "\n",
    "In Figure 2, you can find the pseudo-code for the forward propagation of GraphSAGE. Basically, in each layer, GraphSAGE iterates over all the nodes in the graph and aggregates the neighborhood information from a set of sampled neighbors. Then, different from the original GCN model<sup>2</sup>, the embedding of the current node is concatenated with the aggregated embedding, doubling the size of the embedding vector before applying linear transformation via the learnable parameter $W^k$. After that, the embedding of the current node is updated following the application of a non-linearity.\n",
    "\n",
    "They use different AGGREGATE functions such as mean and max-pooling aggregation. In this question, you need to use mean aggregation.\n",
    "\n",
    "We choose the number of layers as $K=2$ and non-linearity as $ReLU$.\n",
    "\n",
    "It is OK to skip the normalization in the line 7.\n",
    "\n",
    "<img src=\"figures/graphsage_algo.jpg\" alt=\"GraphSAGE Algorithm\" width=\"700\" />\n",
    "\n",
    "(Figure 2: Forward propagation of GraphSAGE<sup>3</sup>)\n",
    "\n",
    "<sup>2</sup>*Kipf, T. N., & Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.*\n",
    "\n",
    "<sup>3</sup>*Liu, X., Yan, M., Deng, L., Li, G., Ye, X., & Fan, D. (2021). Sampling methods for efficient training of graph convolutional networks: A survey. IEEE/CAA Journal of Automatica Sinica, 9(2), 205-234.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kcyM2ioCR939",
   "metadata": {
    "id": "kcyM2ioCR939"
   },
   "outputs": [],
   "source": [
    "class GraphSAGEConvLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GraphSAGEConvLayer, self).__init__()\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # Define the learnable parameter W to be used in linear transformation\n",
    "        # Take into account the dimension increase resulting from the concatenation\n",
    "        # of the aggregated neighbor embeddings with the current node embedding.\n",
    "        #\n",
    "        # self.W = ...\n",
    "        ####################################\n",
    "\n",
    "    def forward(self, curr_node_emb, neighbor_embs):\n",
    "        \"\"\"\n",
    "        Forward pass of a single GraphSAGE convolution layer\n",
    "\n",
    "        Params:\n",
    "        curr_node_emb (Tensor): Embedding vector of the current node.\n",
    "        neighbor_embs (Tensor): Embedding vectors of sampled neighbors of the current node\n",
    "\n",
    "        Returns:\n",
    "        Tensor: New embedding of the current node\n",
    "        \"\"\"\n",
    "\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # 1. Aggregate neighbor embeddings using mean aggregation\n",
    "        # 2. Concatenate the aggregated embeddings with the embedding of the current node\n",
    "        # 3. Apply linear transformation using self.W\n",
    "        # 4. Apply ReLU non-linearity\n",
    "        # 5. Return the new_embedding\n",
    "        ####################################\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphSAGEConvLayer(input_dim, hidden_dim),\n",
    "            GraphSAGEConvLayer(hidden_dim, num_classes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        \"\"\"\n",
    "        Forward pass of GraphSAGE\n",
    "\n",
    "        Params:\n",
    "        X (Tensor): Node feature matrix of shape (N x d)\n",
    "        A (Tensor): Adj. matrix of shape (N x N)\n",
    "        where N is the number of nodes and d is the embedding size.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The output matrix of the last layer with shape (N x num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        ########## YOUR CODE HERE ##########\n",
    "        # 1. For each layer:\n",
    "        #   1.1. Sample neighbors using the sampler function and adj matrix A\n",
    "        #   1.2. Update the embedding for each node\n",
    "        #     1.2.1 Forward pass through the GraphSAGE convolution layer\n",
    "        #     1.2.2 Store the new embeddings for each node\n",
    "        #   1.3. Update the node feature matrix for the next layer\n",
    "        # 2. Return the final node feature matrix with shape (N x num_classes)\n",
    "        ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upLxt7pwWgXn",
   "metadata": {
    "id": "upLxt7pwWgXn"
   },
   "outputs": [],
   "source": [
    "def trainStepGraphSAGE(model, features, adj, labels, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(features, adj)\n",
    "    train_loss = loss_fn(logits, labels)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fjxyx9j6Wl8m",
   "metadata": {
    "id": "Fjxyx9j6Wl8m"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def testGraphSAGE(model, features, adj, labels):\n",
    "    model.eval()\n",
    "    logits = model(features, adj)\n",
    "\n",
    "    _, predicted = torch.max(logits, 1)\n",
    "\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "woYruDm-Wbrn",
   "metadata": {
    "id": "woYruDm-Wbrn"
   },
   "outputs": [],
   "source": [
    "def trainGraphSAGE(model, train_features, val_features, train_adj, val_adj, train_labels, val_labels, num_epochs=50):\n",
    "    t = time.time()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "    best_model = None\n",
    "    best_valid_acc = 0\n",
    "\n",
    "    for epoch in range(1, 1 + num_epochs):\n",
    "        train_loss = trainStepGraphSAGE(model, train_features, train_adj, train_labels, loss_fn, optimizer)\n",
    "        val_acc = testGraphSAGE(model, val_features, val_adj, val_labels)\n",
    "        if val_acc > best_valid_acc:\n",
    "            best_valid_acc = val_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        \n",
    "        print(f'Epoch: {epoch:02d}, '\n",
    "            f'Train Loss: {train_loss:.4f}, ',\n",
    "            f'Validation Accuracy: {100*val_acc:.4f}%, ',\n",
    "            f'time: {time.time() - t:.4f}s.')\n",
    "    \n",
    "    print(f'best acc_valid: {100*best_valid_acc:.4f}%')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gxNlHZg0XAEE",
   "metadata": {
    "id": "gxNlHZg0XAEE"
   },
   "outputs": [],
   "source": [
    "# feel free to play with hidden_dim :)\n",
    "hidden_dim = 64\n",
    "model = GraphSAGE(num_feats, hidden_dim, num_classes, num_samples=5)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nwJWPpEVaJo8",
   "metadata": {
    "id": "nwJWPpEVaJo8"
   },
   "outputs": [],
   "source": [
    "best_model = trainGraphSAGE(model, train_features, val_features, train_adj, val_adj, train_labels, val_labels, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97776f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should get around 70-75% test accuracy.\n",
    "test_acc = testGraphSAGE(best_model, test_features, test_adj, test_labels)\n",
    "print(f\"Test Accuracy: {100*test_acc:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6t4V24ZpjXJx",
   "metadata": {
    "id": "6t4V24ZpjXJx"
   },
   "source": [
    "# 3) Attention-based aggregation in node classification (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dSBj13VcNy0",
   "metadata": {
    "id": "0dSBj13VcNy0"
   },
   "source": [
    "The objective is to develop two types of aggregation methods: mean aggregation and aggregation by attention.\n",
    "\n",
    "For those who require additional guidance or clarification, it is recommended to revisit the relevant [lecture](https://www.youtube.com/watch?v=zRmzVkidkqA&list=PLug43ldmRSo14Y_vt7S6vanPGh-JpHR7T&index=13) or consult the course [notes](https://drive.google.com/file/d/1p7U1xyW4-5W4ge8gRstUkBGQSY3fKHX6/view).\n",
    "\n",
    "\n",
    "Important Reminder: Please ensure that you execute all the cells in each section in sequence to maintain the integrity of intermediate variables and package imports.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g_df0zRghF1b",
   "metadata": {
    "id": "g_df0zRghF1b"
   },
   "source": [
    "## Constructing Layers for Graph Neural Networks (17 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neppPZh5g_47",
   "metadata": {
    "id": "neppPZh5g_47"
   },
   "source": [
    "Let's begin by creating a dummy dataset that will aid in the development and testing of our Graph Neural Networks (GNNs). This dataset will include a simple graph structure with defined nodes, edges, and node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jju0azWFhevg",
   "metadata": {
    "id": "jju0azWFhevg"
   },
   "outputs": [],
   "source": [
    "def create_dummy_data(n_nodes, n_features):\n",
    "  # Create a random adjacency matrix for an undirected graph\n",
    "  # Use a random integer matrix and make it symmetric\n",
    "  adj = torch.triu(torch.randint(0, 2, (n_nodes, n_nodes)), diagonal=1)\n",
    "  adj = adj + adj.T\n",
    "\n",
    "  # Create random features for each node\n",
    "  adj = adj.type(torch.float)\n",
    "  x = torch.rand(n_nodes, n_features)\n",
    "\n",
    "  return x, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7qDSC9VthjlE",
   "metadata": {
    "id": "7qDSC9VthjlE"
   },
   "outputs": [],
   "source": [
    "x, adj = create_dummy_data(5, 3)\n",
    "print(x.shape)\n",
    "print(adj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "djXNL49_hqyt",
   "metadata": {
    "id": "djXNL49_hqyt"
   },
   "source": [
    "### Building a Graph Neural Network with Mean Aggregation (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e_NRLPPhhs_v",
   "metadata": {
    "id": "e_NRLPPhhs_v"
   },
   "source": [
    "Observe that the GNN utilizing a mean aggregator is significantly influenced by how the adjacency matrix is normalized.\n",
    "\n",
    "The formula for the next layer's node representations is given by:\n",
    "\n",
    "$H_{k+1} = a[\\beta_k\\mathbf{1}^T + \\Omega_kH_k(AD^{-1}+I)]$\n",
    "\n",
    "We will proceed to implement the mean normalization of the adjacency matrix using the following expression:\n",
    "\n",
    "$\\widetilde{A}=AD^{-1}+I$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DqOma83HjBth",
   "metadata": {
    "id": "DqOma83HjBth"
   },
   "outputs": [],
   "source": [
    "def mean_normalization(A):\n",
    "  ############# Your code here ############\n",
    "  ## Note:\n",
    "  ## 1. Calculate the degree matrix\n",
    "  ## 2. Create the inverse of the degree matrix\n",
    "  ## 3. Compute the mean normalization of the adjacency matrix\n",
    "  ## (~3 lines of code)\n",
    "\n",
    "  #########################################\n",
    "  return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wSLQfIfjjChw",
   "metadata": {
    "id": "wSLQfIfjjChw"
   },
   "outputs": [],
   "source": [
    "## Test your implementation to observe the behavior of the\n",
    "## mean normalization adjacency matrix.\n",
    "## Note:\n",
    "## It should reflect the values normalized over the number of neighbors,\n",
    "## with the inclusion of a self-loop for each node\n",
    "mean_normalization(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpq54RcAlyHm",
   "metadata": {
    "id": "gpq54RcAlyHm"
   },
   "source": [
    "Now, let's build the GCN layer with a mean aggregator. Remeber, node features are computed as follows:\n",
    "\n",
    "$H_{k+1} = a[\\beta_k\\mathbf{1}^T + \\Omega_kH_k(AD^{-1}+I)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D2gWjKtDmCUW",
   "metadata": {
    "id": "D2gWjKtDmCUW"
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of GCN layer.\n",
    "    It aggregates information from a node's neighbors\n",
    "    using mean aggregation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, activation=None):\n",
    "        super(GCN, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        self.activation = activation\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN layer.\n",
    "\n",
    "        Parameters:\n",
    "        input (Tensor): The input features of the nodes.\n",
    "        adj (Tensor): The adjacency matrix of the graph.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The output features of the nodes after applying the GCN layer.\n",
    "        \"\"\"\n",
    "        adj_norm = mean_normalization(adj)\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Apply the linear transformation\n",
    "        ## 2. Perform the graph convolution operation\n",
    "        ## Note: rename the last line as `output`\n",
    "        ## (2 lines of code)\n",
    "\n",
    "        #########################################\n",
    "        h = self.activation(output) if self.activation else output\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YzuoVNnenV93",
   "metadata": {
    "id": "YzuoVNnenV93"
   },
   "outputs": [],
   "source": [
    "## Ensure that your implementation is flexible enough to accommodate changes\n",
    "## in the number of hidden features. Verify that the dimensions of all\n",
    "## matrices are correctly aligned, allowing for a successful forward\n",
    "## pass through the network.\n",
    "GCN(x.size(1), 3)(x, adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0g-hfZN1pHCo",
   "metadata": {
    "id": "0g-hfZN1pHCo"
   },
   "source": [
    "### Developing a Graph Neural Network with Attention-Based Aggregation (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uEVWsPIYruLD",
   "metadata": {
    "id": "uEVWsPIYruLD"
   },
   "source": [
    "The transformed node embeddings $H_k^{'}$ are calculated using the formula:\n",
    "\n",
    "\\begin{equation}\n",
    "H_k^{'} = \\beta_k\\mathbf{1}^T + \\Omega_kH_k\n",
    "\\end{equation}\n",
    "\n",
    "In this equation, $\\beta_k$ and $\\Omega_k$ are parameters, and $H_k$ represents the node embeddings at layer $k$.\n",
    "\n",
    "To compute the similarity $s_{mn}$ between any two transformed node embeddings $h^{'}_m$ and $h^{'}_n$, we concatenate these embeddings and then take a dot product with a learned parameter vector $\\phi_k$. An activation function is then applied to this dot product:\n",
    "\n",
    "\\begin{equation}\n",
    "s_{mn} = a\\left[\\phi_k^T \\begin{bmatrix} h^{'}_m\\\\ h^{'}_n \\end{bmatrix}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "These similarity values are organized into an $N \\times N$ matrix $S$, where each element represents the similarity between every pair of nodes.\n",
    "\n",
    "The attention weights that contribute to each output embedding are normalized to ensure they are positive and sum to one.\n",
    "\n",
    "This normalization is achieved using the softmax operation. However, it's important to note that only the values corresponding to a node and its neighbors are considered in this computation. The attention weights are then applied to the transformed embeddings as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "a[H_k^{'} * \\text{Softmax}(S, A+I)]\n",
    "\\end{equation}\n",
    "\n",
    "Here, $A+I$ represents the adjacency matrix with added self-loops, ensuring that each node also considers itself when computing attention weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4nXvTZUvvN62",
   "metadata": {
    "id": "4nXvTZUvvN62"
   },
   "source": [
    "To get started, it's important to grasp how to calculate the similarity matrix.\n",
    "\n",
    "A straightforward approach would be to iterate through all the nodes and compute the similarity scores for each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wfJx30JZpGXp",
   "metadata": {
    "id": "wfJx30JZpGXp"
   },
   "outputs": [],
   "source": [
    "N = x.size(0)\n",
    "D = 3\n",
    "\n",
    "# Initialize H' and phi with random values\n",
    "H = torch.rand(size=(N, D))\n",
    "phi = torch.rand(size=(2 * D,))\n",
    "\n",
    "# Initialize the similarity matrix S\n",
    "S = torch.zeros((N, N))\n",
    "\n",
    "# Loop over all nodes to compute the similarity scores\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        ############# Your code here ############\n",
    "        ## 1. Concatenate the features of nodes i and j\n",
    "        ## 2. Compute the dot product of concatenated features with phi\n",
    "        ## (2 lines of code)\n",
    "\n",
    "        #########################################\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Td_GQQX4wiUl",
   "metadata": {
    "id": "Td_GQQX4wiUl"
   },
   "source": [
    "It's crucial to apply a mask to the pre-attention scores before they are processed through the softmax function. This ensures that the normalization is applied exclusively to the existing edges in the graph, maintaining the integrity of the graph structure.\n",
    "\n",
    "\n",
    "Construct the mask using the following equation:\n",
    "$mask = A+I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_Tr83bLQwz6L",
   "metadata": {
    "id": "_Tr83bLQwz6L"
   },
   "outputs": [],
   "source": [
    "############# Your code here ############\n",
    "## contruct the mask, name it `mask`\n",
    "## return a boolean matrix NxN\n",
    "## (1 line of code)\n",
    "\n",
    "#########################################\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4PCNdExKxVb2",
   "metadata": {
    "id": "4PCNdExKxVb2"
   },
   "source": [
    "Apply the mask to the pre-attention\n",
    "$S[mask]$.\n",
    "\n",
    "Set $S$ to very large negative values. This is effectively to represent negative infinity in the context of the softmax operation that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Gzfru1H4w3jX",
   "metadata": {
    "id": "Gzfru1H4w3jX"
   },
   "outputs": [],
   "source": [
    "############# Your code here ############\n",
    "## get masked values for S, `S_masked`\n",
    "## hint: see torch.where\n",
    "## Note: The values masked should effectively be zero,\n",
    "## considering the limits of numerical precision.\n",
    "## (1 line of code)\n",
    "\n",
    "#########################################\n",
    "print(S_masked.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcNf5fYB1E21",
   "metadata": {
    "id": "fcNf5fYB1E21"
   },
   "source": [
    "\n",
    "Now, let's proceed to implement the Graph Attention Network (GAT). The preparatory work we've done should have provided you with all the necessary tools and understanding to successfully implement the attention based aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hMECcwy7pV6B",
   "metadata": {
    "id": "hMECcwy7pV6B"
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation of the GAT layer.\n",
    "\n",
    "    This layer applies an attention mechanism in the graph convolution process,\n",
    "    allowing the model to focus on different parts of the neighborhood\n",
    "    of each node.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, activation):\n",
    "        super(GAT, self).__init__()\n",
    "        # Initialize the weights, bias, and attention parameters as\n",
    "        # trainable parameters\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        self.phi = nn.Parameter(torch.FloatTensor(2 * out_features, 1))\n",
    "        self.activation = activation\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / np.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        stdv = 1. / np.sqrt(self.phi.size(1))\n",
    "        self.phi.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        \"\"\"\n",
    "        Forward pass of the GAT layer.\n",
    "\n",
    "        Parameters:\n",
    "        input (Tensor): The input features of the nodes.\n",
    "        adj (Tensor): The adjacency matrix of the graph.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The output features of the nodes after applying the GAT layer.\n",
    "        \"\"\"\n",
    "        ############# Your code here ############\n",
    "        ## 1. Apply linear transformation and add bias\n",
    "        ## 2. Compute the attention scores utilizing the previously\n",
    "        ## established mechanism.\n",
    "        ## Note: Keep in mind that employing matrix notation can\n",
    "        ## optimize this process.\n",
    "        ## 3. Compute mask based on adjacency matrix\n",
    "        ## 4. Apply mask to the pre-attention matrix\n",
    "        ## 5. Compute attention weights using softmax\n",
    "        ## 6. Aggregate features based on attention weights\n",
    "        ## Note: name the last line as `h`\n",
    "        ## (9-10 lines of code)\n",
    "\n",
    "        #########################################\n",
    "        return self.activation(h) if self.activation else h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OXVpydRKpV3m",
   "metadata": {
    "id": "OXVpydRKpV3m"
   },
   "outputs": [],
   "source": [
    "## Ensure that your implementation is flexible enough to accommodate changes\n",
    "## in the number of hidden features. Verify that the dimensions of all\n",
    "## matrices are correctly aligned, allowing for a successful forward\n",
    "## pass through the network.\n",
    "GCN(x.size(1), 3)(x, adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rtH8rckF2hLR",
   "metadata": {
    "id": "rtH8rckF2hLR"
   },
   "source": [
    "## Node Classification (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B3IdKBlv2ulI",
   "metadata": {
    "id": "B3IdKBlv2ulI"
   },
   "source": [
    "Here is a GNN for node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wSemM4ZJ2tkB",
   "metadata": {
    "id": "wSemM4ZJ2tkB"
   },
   "outputs": [],
   "source": [
    "class NodeClassifier(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, gnn_layer):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "\n",
    "        self.gc1 = gnn_layer(nfeat, nhid, F.relu)\n",
    "        self.gc2 = gnn_layer(nhid, nclass, None)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IPhzOUiNazmc",
   "metadata": {
    "id": "IPhzOUiNazmc"
   },
   "source": [
    "Let's get hands-on experience by loading a benchmark dataset - the Cora dataset.\n",
    "\n",
    "Dataset: Cora is a widely-used benchmark in graph ML. It consists of a citation network where nodes represent scientific papers, and edges correspond to citations between these papers. Each paper (node) is described by a textual abstract and is categorized into one of several classes based on its content.\n",
    "\n",
    "Node Classification: we'll dive into  node classification where the goal is to predict the category of each paper in the Cora citation network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E7x9tcgEZe3A",
   "metadata": {
    "id": "E7x9tcgEZe3A"
   },
   "outputs": [],
   "source": [
    "def load_cora_data(subset_size=100):\n",
    "    # Load the dataset\n",
    "    dataset = CoraGraphDataset()\n",
    "    g = dataset[0]\n",
    "    if subset_size > 0:\n",
    "      # Ensure subset_size is smaller than the total number of nodes\n",
    "      total_nodes = g.num_nodes()\n",
    "      subset_size = min(subset_size, total_nodes)\n",
    "\n",
    "      # Select a subset of nodes\n",
    "      subset_nodes = torch.randperm(total_nodes)[:subset_size]\n",
    "\n",
    "      # Create a subgraph with the selected nodes\n",
    "      subg = g.subgraph(subset_nodes)\n",
    "    else:\n",
    "      subg = g\n",
    "\n",
    "    # Extract features, labels, and masks for the graph\n",
    "    features = subg.ndata['feat']\n",
    "    labels = subg.ndata['label']\n",
    "    train_mask = subg.ndata['train_mask']\n",
    "    val_mask = subg.ndata['val_mask']\n",
    "    test_mask = subg.ndata['test_mask']\n",
    "\n",
    "    adj = subg.adjacency_matrix().to_dense()\n",
    "\n",
    "    return features, labels, adj, train_mask, val_mask, test_mask\n",
    "features, labels, adj, train_mask, val_mask, test_mask = load_cora_data(-1)\n",
    "\n",
    "features = features.to(device)\n",
    "labels = labels.to(device)\n",
    "adj = adj.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "val_mask = val_mask.to(device)\n",
    "test_mask = test_mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "za5azmEYvHMZ",
   "metadata": {
    "id": "za5azmEYvHMZ"
   },
   "source": [
    "Feel free to analyze and print the dimensions of *features, adj*, and *labels*. This will help you debug in case of errors.\n",
    "\n",
    "Note: you can take a subset of the full graph `load_cora_data(subset_size=1000)` to spead up development. However, we expect the final submission on the full graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3g_YeKQUTA0y",
   "metadata": {
    "id": "3g_YeKQUTA0y"
   },
   "source": [
    "Let's define metrics to track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wvR11ZbbTEu4",
   "metadata": {
    "id": "wvR11ZbbTEu4"
   },
   "outputs": [],
   "source": [
    "def calculate_specificity(y_true, y_pred, labels):\n",
    "    specificity_scores = np.zeros(len(labels))\n",
    "    for i, label in enumerate(labels):\n",
    "        binary_true = (y_true == label).int()\n",
    "        binary_pred = (y_pred == label).int()\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(binary_true, binary_pred).ravel()\n",
    "\n",
    "        specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "        specificity_scores[i] = specificity\n",
    "\n",
    "    return np.mean(specificity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piwFKneJ4BNi",
   "metadata": {
    "id": "piwFKneJ4BNi"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(target, prediction):\n",
    "  ############# Your code here ############\n",
    "  ## 1. Count the number of correct predictions\n",
    "  ## 2. Get the total number of predictions\n",
    "  ## 3. Calculate the accuracy\n",
    "  ## (~3 lines of code)\n",
    "\n",
    "  #########################################\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AFZVCe7DUNBV",
   "metadata": {
    "id": "AFZVCe7DUNBV"
   },
   "outputs": [],
   "source": [
    "sensitivity = lambda y_true, y_pred: recall_score(y_true, y_pred, average='macro')\n",
    "specificity = lambda y_true, y_pred: calculate_specificity(y_true, y_pred, labels.unique())\n",
    "accuracy = lambda y_true, y_pred: compute_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HHzjkKv4vwA2",
   "metadata": {
    "id": "HHzjkKv4vwA2"
   },
   "source": [
    "Almost there, write the main training function to train our GNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BDvI2ZD4gQOj",
   "metadata": {
    "id": "BDvI2ZD4gQOj"
   },
   "outputs": [],
   "source": [
    "def train_step(model, X, A, y ,train_mask, optimizer, loss_fn):\n",
    "  model.train()\n",
    "  loss = 0\n",
    "  ############# Your code here ############\n",
    "  ## 1. Zero grad the optimizer\n",
    "  ## 2. Feed the data into the model\n",
    "  ## 3. Slice the model output and label by train_mask\n",
    "  ## 4. Feed the sliced output and label to loss_fn\n",
    "  ## (~4 lines of code)\n",
    "\n",
    "  #########################################\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zDBF9eMe31sg",
   "metadata": {
    "id": "zDBF9eMe31sg"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, X, A, y, train_mask, val_mask, test_mask=None, metrics={}):\n",
    "  model.eval()\n",
    "\n",
    "  # The output of model on all data\n",
    "  out = None\n",
    "  logits = model(X, A)\n",
    "  preds = logits.argmax(dim=1)\n",
    "\n",
    "  result = {\n",
    "        'train': {},\n",
    "        'val': {},\n",
    "      }\n",
    "  if test_mask is not None:\n",
    "    result['test'] = {}\n",
    "\n",
    "  for name, metric in metrics.items():\n",
    "    result['train'][name] = metric(y[train_mask], preds[train_mask])\n",
    "    result['val'][name] = metric(y[val_mask], preds[val_mask])\n",
    "\n",
    "    if test_mask is not None:\n",
    "      result['test'][name] = metric(y[test_mask], preds[test_mask])\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RJNEQbtB4xQY",
   "metadata": {
    "id": "RJNEQbtB4xQY"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, lr):\n",
    "  t = time.time()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=lr,\n",
    "                            weight_decay=5e-4)\n",
    "  loss_fn = F.nll_loss\n",
    "  metrics = {\"acc\": accuracy}\n",
    "\n",
    "  best_model = None\n",
    "  best_valid_acc = 0\n",
    "\n",
    "  for epoch in range(1, 1 + epochs):\n",
    "    loss = train_step(model, features, adj, labels, train_mask,\n",
    "                      optimizer, loss_fn)\n",
    "\n",
    "\n",
    "    result = test(model, features, adj, labels,\n",
    "                train_mask, val_mask, None, metrics)\n",
    "    train_acc = result['train']['acc']\n",
    "    valid_acc = result['val']['acc']\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "          f'Loss: {loss:.4f}, ',\n",
    "          f'acc_train: {100*train_acc:.4f}%, ',\n",
    "          f'acc_valid: {100*valid_acc:.4f}%, ',\n",
    "          f'time: {time.time() - t:.4f}s.')\n",
    "  print(f'best acc_valid: {100*best_valid_acc:.4f}%')\n",
    "  return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7pGPt9EB9kD9",
   "metadata": {
    "id": "7pGPt9EB9kD9"
   },
   "source": [
    "Execute node classification on the Cora dataset using mean aggregation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcgU70zI8KXD",
   "metadata": {
    "id": "bcgU70zI8KXD"
   },
   "outputs": [],
   "source": [
    "## Note:\n",
    "## feel free to play with the parameters to improve validation accuracy\n",
    "## you should get a validationaccuracy of at least 77%-80%\n",
    "model_GCN = NodeClassifier(nfeat=features.shape[1],\n",
    "                  nhid=16,\n",
    "                  nclass=labels.max().item() + 1,\n",
    "                  dropout=0.5,\n",
    "                  gnn_layer=GCN).to(device)\n",
    "model_GCN = train(model_GCN, 100, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8M_Shomo95Xq",
   "metadata": {
    "id": "8M_Shomo95Xq"
   },
   "source": [
    "Execute node classification on the Cora dataset using attention based aggregation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SaZjU5tn4yGk",
   "metadata": {
    "id": "SaZjU5tn4yGk"
   },
   "outputs": [],
   "source": [
    "## Note:\n",
    "## feel free to play with the parameters to improve validation accuracy\n",
    "## you should get a validationaccuracy of at least 77%-80%\n",
    "model_GAT = NodeClassifier(nfeat=features.shape[1],\n",
    "              nhid=16,\n",
    "              nclass=labels.max().item() + 1,\n",
    "              dropout=0.5,\n",
    "              gnn_layer=GAT).to(device)\n",
    "model_GAT = train(model_GAT, 100, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jAqFqoZPUTHN",
   "metadata": {
    "id": "jAqFqoZPUTHN"
   },
   "source": [
    "## Visualize and analize results (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-mAEk0w8ZiMy",
   "metadata": {
    "id": "-mAEk0w8ZiMy"
   },
   "source": [
    "\n",
    "Familiarize yourself with the following utility functions, which are designed for visual analysis and will assist us in evaluating our GNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_4lCscL3U1GG",
   "metadata": {
    "id": "_4lCscL3U1GG"
   },
   "outputs": [],
   "source": [
    "def plot_embeddings(embeddings1, embeddings2, labels):\n",
    "    # Convert multi-labels to unique integers for color-coding\n",
    "    unique_labels, label_indices = np.unique(labels, axis=0, return_inverse=True)\n",
    "\n",
    "    # Set up the matplotlib figure and axes\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Perform PCA for both sets of embeddings\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result1 = pca.fit_transform(embeddings1)\n",
    "    pca_result2 = pca.fit_transform(embeddings2)\n",
    "\n",
    "    # Plotting GCN\n",
    "    scatter1 = axs[0].scatter(pca_result1[:, 0], pca_result1[:, 1], c=label_indices, cmap='viridis')\n",
    "    axs[0].set_title('GCN')\n",
    "    axs[0].set_xlabel('PCA Component 1')\n",
    "    axs[0].set_ylabel('PCA Component 2')\n",
    "\n",
    "    # Plotting GAT\n",
    "    scatter2 = axs[1].scatter(pca_result2[:, 0], pca_result2[:, 1], c=label_indices, cmap='viridis')\n",
    "    axs[1].set_title('GAT')\n",
    "    axs[1].set_xlabel('PCA Component 1')\n",
    "    axs[1].set_ylabel('PCA Component 2')\n",
    "\n",
    "    # Create a color bar with label information\n",
    "    cbar = plt.colorbar(scatter1, ax=axs, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Label Combinations')\n",
    "    cbar.set_ticks(np.arange(len(unique_labels)))\n",
    "    cbar.set_ticklabels([' + '.join(str(comb)) for comb in unique_labels])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4XlA1WI7YsnC",
   "metadata": {
    "id": "4XlA1WI7YsnC"
   },
   "outputs": [],
   "source": [
    "def plot_weight_distributions(weights1, weights2):\n",
    "    # Flatten the weight matrices\n",
    "    flattened_weights1 = weights1.flatten()\n",
    "    flattened_weights2 = weights2.flatten()\n",
    "\n",
    "    # Create KDE plots\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(flattened_weights1, fill=True, color=\"r\", label=\"GCN\")\n",
    "    sns.kdeplot(flattened_weights2, fill=True, color=\"b\", label=\"GAT\")\n",
    "\n",
    "    plt.title(\"Distribution of GNN Weights\")\n",
    "    plt.xlabel(\"Weight Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uC6QMjPadcfk",
   "metadata": {
    "id": "uC6QMjPadcfk"
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, title, phases = ['train', 'val', 'test']):\n",
    "    n_phases = len(phases)\n",
    "\n",
    "    # Setting up the subplot grid\n",
    "    fig, axs = plt.subplots(n_phases, 1, figsize=(15, 6 * n_phases), sharey=True)\n",
    "\n",
    "    for i, phase in enumerate(phases):\n",
    "        # Extracting metric names\n",
    "        metric_names = list(metrics[0][phase].keys())\n",
    "\n",
    "        # Number of metrics\n",
    "        n_metrics = len(metric_names)\n",
    "\n",
    "        # Data for plotting\n",
    "        scores_modelA = [metrics[0][phase][metric] for metric in metric_names]\n",
    "        scores_modelB = [metrics[1][phase][metric] for metric in metric_names]\n",
    "\n",
    "        # Setting the positions and width for the bars\n",
    "        pos = np.arange(n_metrics)\n",
    "        bar_width = 0.35\n",
    "\n",
    "        # Plotting in the respective subplot\n",
    "        axs[i].bar(pos - bar_width/2, scores_modelA, bar_width, label='GCN')\n",
    "        axs[i].bar(pos + bar_width/2, scores_modelB, bar_width, label='GAT')\n",
    "\n",
    "        # Adding labels and titles\n",
    "        axs[i].set_xlabel('Metrics')\n",
    "        axs[i].set_title(f'{phase.capitalize()} Metrics')\n",
    "        axs[i].set_xticks(pos)\n",
    "        axs[i].set_xticklabels(metric_names)\n",
    "\n",
    "        # Adding a legend to the first subplot\n",
    "        if i == 0:\n",
    "            axs[i].legend()\n",
    "\n",
    "    # Setting the main title and showing the plot\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "di6Zhiq0ds3p",
   "metadata": {
    "id": "di6Zhiq0ds3p"
   },
   "source": [
    "### Node Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hnPNAFYLFORu",
   "metadata": {
    "id": "hnPNAFYLFORu"
   },
   "outputs": [],
   "source": [
    "############# Your code here ############\n",
    "## 1. Generate embeddings using the GCN model\n",
    "## 2. Generate embeddings using the GAT model\n",
    "## Note: use detach to avoid using the computation graph\n",
    "## convert the tensor to a numpy array\n",
    "## use H_GCN and H_GAT as variable names\n",
    "## (~2 lines of code)\n",
    "\n",
    "#########################################\n",
    "plot_embeddings(H_GCN,H_GAT, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qgz-LV0laj5U",
   "metadata": {
    "id": "qgz-LV0laj5U"
   },
   "source": [
    "Based on the visualizations in the plots, what insights can you gather about the embeddings generated by each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PSbW4t-Ed6kr",
   "metadata": {
    "id": "PSbW4t-Ed6kr"
   },
   "source": [
    "######## Your response here (double-click) ########\n",
    "\n",
    "\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5NkRAVHefKI",
   "metadata": {
    "id": "b5NkRAVHefKI"
   },
   "source": [
    "### Distribution of learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcXWvuy0Yc7n",
   "metadata": {
    "id": "bcXWvuy0Yc7n"
   },
   "outputs": [],
   "source": [
    "############# Your code here ############\n",
    "## 1. Extract the weights from the second graph\n",
    "## convolutional layer (gc2) of the GCN model\n",
    "## 2. Extract the weights from the second graph\n",
    "## convolutional layer (gc2) of the GAT model\n",
    "## Note: use detach to avoid using the computation graph\n",
    "## convert the tensor to a numpy array\n",
    "## use Omega_GCN_gc2 and Omega_GAT_gc2 as variable names\n",
    "## (~2 lines of code)\n",
    "\n",
    "#########################################\n",
    "plot_weight_distributions(Omega_GCN_gc2, Omega_GAT_gc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uffhAF6Ie9iD",
   "metadata": {
    "id": "uffhAF6Ie9iD"
   },
   "source": [
    "Reflect on the observed distributions of the weights from the models. What conclusions or understandings can be drawn about the behavior and characteristics of each model based on these distributions?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Poj0JkdBe9vV",
   "metadata": {
    "id": "Poj0JkdBe9vV"
   },
   "source": [
    "######## Your response here (double-click) ########\n",
    "\n",
    "\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WwhgePeTe4tK",
   "metadata": {
    "id": "WwhgePeTe4tK"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TjhJAM3GbkWW",
   "metadata": {
    "id": "TjhJAM3GbkWW"
   },
   "outputs": [],
   "source": [
    "metrics = {\"acc\": accuracy, \"sensitivity\": sensitivity, \"specifity\": specificity}\n",
    "results_GCN = test(model_GCN, features, adj,\n",
    "                   labels, train_mask, val_mask, test_mask, metrics)\n",
    "results_GAT = test(model_GAT, features, adj,\n",
    "                   labels, train_mask, val_mask, test_mask, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26gMjOincTMm",
   "metadata": {
    "id": "26gMjOincTMm"
   },
   "outputs": [],
   "source": [
    "plot_metrics([results_GCN, results_GAT], 'Metric Comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ArHRsbJ7gv9t",
   "metadata": {
    "id": "ArHRsbJ7gv9t"
   },
   "source": [
    "How might you interpret the outcomes of these results?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hWjw5eD7gmx7",
   "metadata": {
    "id": "hWjw5eD7gmx7"
   },
   "source": [
    "######## Your response here (double-click) ########\n",
    "\n",
    "\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qIc4lj0whLbW",
   "metadata": {
    "id": "qIc4lj0whLbW"
   },
   "source": [
    "## Bonus question 1\n",
    "\n",
    "What changes in the GCN and GAT layers would result in a more effective implementation?\n",
    "\n",
    "Hint: See how [message passing](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html) is implemented in PyTorch Geometric.\n",
    "\n",
    "Furthermore, explore the specific implementations for [GCN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html) and [GAT](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html).\n",
    "\n",
    "*Note: Bonus questions are ungraded yet provide an opportunity for those who wish to delve deeper into their graph based learning journey.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q4uW3ekljtlQ",
   "metadata": {
    "id": "Q4uW3ekljtlQ"
   },
   "source": [
    "######## Your response here (double-click) ########\n",
    "\n",
    "\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahE7TBzSwKCt",
   "metadata": {
    "id": "ahE7TBzSwKCt"
   },
   "source": [
    "## Bonus question 2\n",
    "\n",
    "Write the kipf normalization\n",
    "\n",
    "$H_{k+1} = a[\\beta_k\\mathbf{1}^T + \\Omega_kH_k(D^{-1/2}AD^{-1/2}+I)]$\n",
    "\n",
    "*Note: Bonus questions are ungraded yet provide an opportunity for those who wish to delve deeper into their graph based learning journey.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IQ_VTX4Awo7n",
   "metadata": {
    "id": "IQ_VTX4Awo7n"
   },
   "outputs": [],
   "source": [
    "def kipf_normalization(A):\n",
    "  ############# Your code here ############\n",
    "\n",
    "  #########################################\n",
    "  return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gaf3LOV7c5O9",
   "metadata": {
    "id": "Gaf3LOV7c5O9"
   },
   "source": [
    "# 4) Propagation rule integrating edge features/embeddings (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aCStFPrA8rAD",
   "metadata": {
    "id": "aCStFPrA8rAD"
   },
   "outputs": [],
   "source": [
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)  # Setting a fixed seed for numpy random number generator\n",
    "\n",
    "\n",
    "# we will create a stochastic block model graph with two communities and 20 nodes\n",
    "G = nx.stochastic_block_model(sizes=[10, 10], p=[[0.8, 0.1], [0.1, 0.8]], seed=seed)\n",
    "\n",
    "# We will create a node feature\n",
    "communities = nx.community.louvain_communities(G)\n",
    "X = np.zeros((len(G), 1))\n",
    "X[list(communities[0])] = 1\n",
    "\n",
    "A = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "# we will use powers of the adjacency matrix as edge features (considering edge embeddings only on edges already existing)\n",
    "E1 = A\n",
    "E2 = (A @ A)*A # we zero the embedding on edges which aren't connected\n",
    "E3 = (A @ A @ A)*A\n",
    "E4 = (A @ A @ A @ A)*A\n",
    "\n",
    "E = np.stack((E1, E2, E3, E4), axis = 2)\n",
    "\n",
    "print(E)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3EPth4u885N-",
   "metadata": {
    "id": "3EPth4u885N-"
   },
   "source": [
    "### 4.1) Graph visualization <b>(5 points)</b>\n",
    "\n",
    "#### 4.1.1 Use networkx to visualize the structure and the node features of the graph. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svQEBbzg8xX_",
   "metadata": {
    "id": "svQEBbzg8xX_"
   },
   "outputs": [],
   "source": [
    "def VisualizeGraphNetworkx(G):\n",
    "    '''\n",
    "    input: G\n",
    "    A networkx graph instance\n",
    "    '''\n",
    "    #Complete code here:\n",
    "    ####################\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2BRqCmSV9CxU",
   "metadata": {
    "id": "2BRqCmSV9CxU"
   },
   "source": [
    "#### 4.1.2 Display the edge features for edges which are connected to node 0 (first node). (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GAa3JxT79E_7",
   "metadata": {
    "id": "GAa3JxT79E_7"
   },
   "outputs": [],
   "source": [
    "''' code to display the edge features for all edges connected to node 0. List the output in the format\n",
    "[node 0, node x, edge_features]\n",
    "[node 0, node y, edge_features] ...\n",
    " Please change = None to your implementation.'''\n",
    "edge_features = None\n",
    "print(edge_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oqL_AF0m9JCD",
   "metadata": {
    "id": "oqL_AF0m9JCD"
   },
   "source": [
    "### 4.2 Mathematical Formulation <b>(5 points)</b>\n",
    "\n",
    "Consider a graph with node embedding $\\mathbf{h}_n$ based on its neighboring node embeddings $\\{\\mathbf{h}_m\\}_{m \\in ne[n]}$ and the neighboring edge embeddings $\\{\\mathbf{e}_m\\}_{m \\in nee[n]}$\n",
    "$ne[n]$ denotes the neighbors of node $n$ and $nee[n]$ denotes the edges connected to node $n$.\n",
    "  \n",
    "Let the update rule without accounting for edge embeddings is the following\n",
    "\n",
    "$\\mathbf{H}_{k+1} = a[ \\mathbf{\\beta}_k \\mathbf{1}^T + \\mathbf{\\Omega}_k \\mathbf{H}_k (\\mathbf{A} + \\mathbf{I}) ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0wTK6G6X9au5",
   "metadata": {
    "id": "0wTK6G6X9au5"
   },
   "source": [
    "#### 4.2.1 fill in the [...] in the equation below so that we account for edge embeddings (3 points)\n",
    "\n",
    "$\\mathbf{H}_{k+1} = a[ \\mathbf{\\beta}_k \\mathbf{1}^T + \\mathbf{\\Omega}_k \\mathbf{H}_k (\\mathbf{A} + \\mathbf{I}) + [...]]$  (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-o0WJog-9cYa",
   "metadata": {
    "id": "-o0WJog-9cYa"
   },
   "source": [
    "#### 4.2.2 Define any new variables you introduce in the matrix form of (1) and specify their size. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FO84ej4M9e9S",
   "metadata": {
    "id": "FO84ej4M9e9S"
   },
   "source": [
    "### 4.3) Implementation <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qtFGBFbG9h5B",
   "metadata": {
    "id": "qtFGBFbG9h5B"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "np.random.seed(42)  # Setting a fixed seed for numpy random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def add_self_connections(A):\n",
    "    \"\"\"\n",
    "    Add self-connections to the adjacency matrix.\n",
    "\n",
    "    This function adds an identity matrix to the adjacency matrix `A`, which\n",
    "    effectively adds a self-loop to each node in the graph. This is a common\n",
    "    preprocessing step in graph neural network implementations.\n",
    "\n",
    "    Parameters:\n",
    "    A (np.ndarray): The adjacency matrix to modify.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The adjacency matrix with self-connections added.\n",
    "    \"\"\"\n",
    "    I = np.eye(A.shape[0])\n",
    "    return A + I\n",
    "\n",
    "\n",
    "def normalize_adjacency(A_hat):\n",
    "    \"\"\"\n",
    "    Normalize the adjacency matrix.\n",
    "\n",
    "    This function applies symmetric normalization to the adjacency matrix\n",
    "    `A_hat`. The normalization is done using the inverse square root of the\n",
    "    degree matrix. This step is important for many graph-based learning\n",
    "    algorithms to ensure that the scale of the feature representations is not\n",
    "    skewed by node degree.\n",
    "\n",
    "    Parameters:\n",
    "    A_hat (np.ndarray): The adjacency matrix with self-connections.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The normalized adjacency matrix.\n",
    "    \"\"\"\n",
    "    D_hat_inv_sqrt = np.diag(1.0 / np.sqrt(np.sum(A_hat, axis=0)))\n",
    "    return D_hat_inv_sqrt.dot(A_hat).dot(D_hat_inv_sqrt)\n",
    "\n",
    "\n",
    "\n",
    "# GCN update steps\n",
    "A_hat = add_self_connections(A) # A is from question 1.2\n",
    "\n",
    "# define input features\n",
    "H_k = X\n",
    "\n",
    "# Define the dimensions of the weight matrix\n",
    "input_features = H_k.shape[1]  # Number of input features (columns of H_k)\n",
    "output_features = 2  # Number of output features (can be chosen based on your model's design)\n",
    "\n",
    "# Initialize the weight matrix Omega_k with random values\n",
    "Omega_k = np.random.rand(input_features, output_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QkMfH2569lAo",
   "metadata": {
    "id": "QkMfH2569lAo"
   },
   "source": [
    "#### 4.1 Implement equation (1) and print out an update using this equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oJasGBjZ9nX5",
   "metadata": {
    "id": "oJasGBjZ9nX5"
   },
   "outputs": [],
   "source": [
    "''' Please implement your update which includes edge embeddings below (change = None)\n",
    "You should use A, X and E defined at the beginning of 4)\n",
    "\n",
    "'''\n",
    "H_k_edge_embeddings = None\n",
    "\n",
    "print(\"\\nWith edge-embeddings:\\n\", H_k_edge_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eqEsEFp8c5O-",
   "metadata": {
    "id": "eqEsEFp8c5O-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Yh_VUCCsc5O-",
   "metadata": {
    "id": "Yh_VUCCsc5O-"
   },
   "source": [
    "# 5) Bonus (5 points)\n",
    "\n",
    "#### Graph Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S0Fzftx0_kNV",
   "metadata": {
    "id": "S0Fzftx0_kNV"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# here we have two datasets of the same digits. One using pixels and the other in the Fourier Domain.\n",
    "\n",
    "# load digits in fourier domain\n",
    "df1 = pd.read_csv('sample_data/fourier.csv', header=None)\n",
    "X1 = df1.to_numpy()\n",
    "print(len(X1))\n",
    "\n",
    "# load digits in pixel domain\n",
    "df2 = pd.read_csv('sample_data/pixel.csv', header=None)\n",
    "X2 = df2.to_numpy()\n",
    "print(len(X2))\n",
    "\n",
    "\n",
    "# generate distances between all pairs of points to create distance matrix\n",
    "A1 = euclidean_distances(X1,X1)\n",
    "A2 = euclidean_distances(X2,X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2VRqe8vn_dPR",
   "metadata": {
    "id": "2VRqe8vn_dPR"
   },
   "source": [
    "Here we have two graphs $G =(A1,X1)$ and $G' = (A2,X2)$ representating the same dataset in different domains.\n",
    "\n",
    "#### 5.1) Use the SNF graph cross-diffusion rule (see equations (4) and (5) in the SNF paper) to define a function $G_f = fuse(G,G')$ that fuses the node and edge features of these two graphs. Formalize the solution mathematically in the matrix form (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_GCv6arK_dnL",
   "metadata": {
    "id": "_GCv6arK_dnL"
   },
   "source": [
    "answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bj2SPVR9_dq0",
   "metadata": {
    "id": "Bj2SPVR9_dq0"
   },
   "source": [
    "#### 5.2) Cross-diffuse graphs G and G' above using your new formula. (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2UREw95M_dy7",
   "metadata": {
    "id": "2UREw95M_dy7"
   },
   "source": [
    "answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3YOZHLJc5O-",
   "metadata": {
    "id": "f3YOZHLJc5O-"
   },
   "source": [
    "#### References:\n",
    "1. SNF paper: Wang, Bo, et al. \"Similarity network fusion for aggregating data types on a genomic scale.\" Nature methods 11.3 (2014): 333-337.\n",
    "2. Feel free to check  <A Href=\"https://www.youtube.com/watch?v=Oqrjkm6TIy8&list=PLug43ldmRSo3MV-Jgjr30E5SpwNKkjTvJ&index=32\">video</A> from minute 21 to 30 (9 minutes to cover)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LQ7SGGtic5O6"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
